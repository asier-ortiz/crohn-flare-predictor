# üîå Gu√≠a de Integraci√≥n - API ML

Esta gu√≠a explica c√≥mo integrar el servicio ML en la aplicaci√≥n web del proyecto.

## üìã Contexto

El servicio ML es **independiente** de la aplicaci√≥n web. Funciona como un microservicio stateless que:
- NO tiene base de datos
- NO gestiona usuarios
- Solo recibe datos, procesa y devuelve predicciones

## üèóÔ∏è Arquitectura de Integraci√≥n

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend (Vue)    ‚îÇ
‚îÇ   localhost:5173    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ HTTP
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend (FastAPI)  ‚îÇ
‚îÇ   localhost:8000    ‚îÇ
‚îÇ                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ml_client.py ‚îÇ   ‚îÇ  ‚Üê Cliente HTTP para llamar al ML API
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ HTTP
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ML API (FastAPI)  ‚îÇ  ‚Üê Este proyecto
‚îÇ   localhost:8001    ‚îÇ
‚îÇ                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Modelos ML   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Setup Inicial

### 1. Verificar que el servicio ML est√° corriendo

```bash
# En una terminal, inicia el servicio ML
cd crohn-flare-predictor
make serve

# Deber√≠a estar en http://localhost:8001
curl http://localhost:8001/health
```

### 2. Configurar variables de entorno en el backend web

```bash
# En crohn-web-app/.env
ML_API_URL=http://localhost:8001
ML_API_TIMEOUT=30
```

## üíª Implementaci√≥n en el Backend Web

### Paso 1: Crear cliente HTTP para ML API

Crea el archivo `crohn-web-app/backend/api/ml_client.py`:

```python
"""
Cliente para comunicarse con el servicio ML.
"""
import httpx
from typing import Dict, Any, List
from fastapi import HTTPException, status
import logging

logger = logging.getLogger(__name__)


class MLAPIClient:
    """Cliente HTTP para el servicio ML."""

    def __init__(self, base_url: str = "http://localhost:8001"):
        self.base_url = base_url
        self.timeout = 30.0

    async def predict_flare(
        self,
        symptoms: Dict,
        demographics: Dict,
        history: Dict
    ) -> Dict[str, Any]:
        """
        Predecir riesgo de brote para un paciente.

        Args:
            symptoms: S√≠ntomas actuales (abdominal_pain, diarrhea, etc.)
            demographics: Datos demogr√°ficos (age, gender, etc.)
            history: Historial m√©dico (previous_flares, medications, etc.)

        Returns:
            {
                "prediction": {
                    "flare_risk": "low|medium|high",
                    "probability": float,
                    "confidence": float
                },
                "factors": {
                    "top_contributors": [...],
                    "symptom_severity_score": float
                },
                "recommendation": str
            }

        Raises:
            HTTPException: Si el servicio ML no est√° disponible
        """
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/predict",
                    json={
                        "symptoms": symptoms,
                        "demographics": demographics,
                        "history": history
                    }
                )
                response.raise_for_status()
                return response.json()

        except httpx.TimeoutException:
            logger.error("ML API timeout")
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail="ML service timeout"
            )
        except httpx.HTTPError as e:
            logger.error(f"ML API error: {e}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="ML service unavailable"
            )

    async def analyze_trends(
        self,
        patient_id: str,
        daily_records: List[Dict]
    ) -> Dict[str, Any]:
        """
        Analizar tendencias de s√≠ntomas en el tiempo.

        Args:
            patient_id: ID del paciente
            daily_records: Lista de registros diarios
                [
                    {
                        "date": "2024-11-01",
                        "symptoms": {...}
                    },
                    ...
                ]

        Returns:
            {
                "patient_id": str,
                "analysis_period": {...},
                "trends": {
                    "overall_trend": "improving|stable|worsening",
                    "severity_change": float,
                    "concerning_patterns": [...]
                },
                "risk_assessment": {...},
                "recommendations": [...]
            }
        """
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/analyze/trends",
                    json={
                        "patient_id": patient_id,
                        "daily_records": daily_records,
                        "window_days": 14
                    }
                )
                response.raise_for_status()
                return response.json()

        except httpx.HTTPError as e:
            logger.error(f"ML API trends error: {e}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="ML service unavailable"
            )

    async def batch_predict(
        self,
        patients: List[Dict]
    ) -> Dict[str, Any]:
        """
        Predicciones por lotes (√∫til para dashboard m√©dico).

        Args:
            patients: Lista de hasta 100 pacientes
                [
                    {
                        "patient_id": str,
                        "symptoms": {...},
                        "demographics": {...},
                        "history": {...}
                    },
                    ...
                ]

        Returns:
            {
                "results": [...],
                "processed_count": int,
                "failed_count": int
            }
        """
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:  # M√°s timeout
                response = await client.post(
                    f"{self.base_url}/predict/batch",
                    json={"patients": patients}
                )
                response.raise_for_status()
                return response.json()

        except httpx.HTTPError as e:
            logger.error(f"ML API batch error: {e}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="ML service unavailable"
            )


# Singleton instance
ml_client = MLAPIClient()
```

### Paso 2: Usar en endpoints del backend web

Ejemplo de c√≥mo usar el cliente en tus endpoints:

```python
# crohn-web-app/backend/api/symptoms.py
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from db.database import get_db
from db.models import User, DailySymptom, PredictionCache
from .ml_client import ml_client
import logging

logger = logging.getLogger(__name__)
router = APIRouter()


@router.post("/symptoms/daily")
async def record_daily_symptoms(
    symptoms: SymptomsInput,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Usuario registra s√≠ntomas del d√≠a.
    1. Guardar en BD
    2. Llamar a ML API para predicci√≥n
    3. Guardar predicci√≥n en cache
    4. Devolver resultado
    """

    # 1. Guardar s√≠ntomas en BD
    symptom_record = DailySymptom(
        user_id=current_user.id,
        symptom_date=date.today(),
        abdominal_pain=symptoms.abdominal_pain,
        diarrhea=symptoms.diarrhea,
        fatigue=symptoms.fatigue,
        fever=symptoms.fever,
        weight_change=symptoms.weight_change,
        blood_in_stool=symptoms.blood_in_stool or False,
        nausea=symptoms.nausea or 0
    )
    db.add(symptom_record)
    db.commit()
    db.refresh(symptom_record)

    # 2. Llamar al servicio ML (puede fallar, no bloquear la app)
    prediction = None
    try:
        ml_prediction = await ml_client.predict_flare(
            symptoms=symptoms.dict(),
            demographics={
                "age": current_user.age,
                "gender": current_user.gender,
                "disease_duration_years": current_user.disease_duration_years,
                "bmi": current_user.bmi
            },
            history={
                "previous_flares": current_user.previous_flares,
                "medications": current_user.medications,
                "last_flare_days_ago": calculate_days_since_flare(current_user),
                "surgery_history": current_user.surgery_history,
                "smoking_status": current_user.smoking_status
            }
        )

        # 3. Guardar predicci√≥n en cache
        prediction_cache = PredictionCache(
            user_id=current_user.id,
            symptom_record_id=symptom_record.id,
            flare_risk=ml_prediction["prediction"]["flare_risk"],
            probability=ml_prediction["prediction"]["probability"],
            confidence=ml_prediction["prediction"]["confidence"],
            recommendation=ml_prediction["recommendation"],
            factors=ml_prediction["factors"]
        )
        db.add(prediction_cache)
        db.commit()

        prediction = ml_prediction

    except Exception as e:
        # Si ML API falla, continuar sin predicci√≥n
        logger.warning(f"ML API unavailable: {e}")
        prediction = None

    # 4. Devolver resultado
    return {
        "symptom_record": symptom_record,
        "prediction": prediction,
        "message": "Symptoms recorded successfully"
    }


@router.get("/trends/{user_id}")
async def get_user_trends(
    user_id: int,
    days: int = 14,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Obtener an√°lisis de tendencias para un usuario."""

    # Verificar permisos
    if current_user.id != user_id and not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Forbidden")

    # Obtener registros diarios
    daily_records = db.query(DailySymptom).filter(
        DailySymptom.user_id == user_id
    ).order_by(DailySymptom.symptom_date.desc()).limit(days).all()

    if len(daily_records) < 7:
        raise HTTPException(
            status_code=400,
            detail="Need at least 7 days of data for trend analysis"
        )

    # Formatear para ML API
    ml_records = [
        {
            "date": record.symptom_date.isoformat(),
            "symptoms": {
                "abdominal_pain": record.abdominal_pain,
                "diarrhea": record.diarrhea,
                "fatigue": record.fatigue,
                "fever": record.fever,
                "weight_change": record.weight_change,
                "blood_in_stool": record.blood_in_stool,
                "nausea": record.nausea or 0
            }
        }
        for record in reversed(daily_records)  # Ordenar cronol√≥gicamente
    ]

    # Llamar al servicio ML
    trend_analysis = await ml_client.analyze_trends(
        patient_id=str(user_id),
        daily_records=ml_records
    )

    return trend_analysis
```

## üìä Schemas de Datos

### Formato de S√≠ntomas

```python
{
    "abdominal_pain": int (0-10),
    "diarrhea": int (0-10),
    "fatigue": int (0-10),
    "fever": bool,
    "weight_change": float,
    "blood_in_stool": bool,
    "nausea": int (0-10)
}
```

### Formato de Demografia

```python
{
    "age": int (0-120),
    "gender": "M" | "F" | "O",
    "disease_duration_years": int,
    "bmi": float (opcional)
}
```

### Formato de Historial

```python
{
    "previous_flares": int,
    "medications": list[str],
    "last_flare_days_ago": int,
    "surgery_history": bool (opcional),
    "smoking_status": "never" | "former" | "current" (opcional)
}
```

## üîÑ Flujos Comunes

### Flujo 1: Registro Diario de S√≠ntomas

```
Usuario completa formulario
    ‚Üì
Frontend ‚Üí POST /api/symptoms/daily (Backend Web)
    ‚Üì
Backend guarda en BD
    ‚Üì
Backend ‚Üí POST /predict (ML API)
    ‚Üì
ML API devuelve predicci√≥n
    ‚Üì
Backend guarda predicci√≥n en cache
    ‚Üì
Backend ‚Üí Frontend (s√≠ntomas + predicci√≥n)
    ‚Üì
Mostrar al usuario
```

### Flujo 2: Ver Tendencias (Login o Dashboard)

```
Usuario hace login / abre dashboard
    ‚Üì
Frontend ‚Üí GET /api/trends/{user_id} (Backend Web)
    ‚Üì
Backend obtiene √∫ltimos 14 d√≠as de BD
    ‚Üì
Backend ‚Üí POST /analyze/trends (ML API)
    ‚Üì
ML API analiza tendencias
    ‚Üì
Backend ‚Üí Frontend (an√°lisis)
    ‚Üì
Mostrar gr√°ficas y alertas
```

## üö® Manejo de Errores

**Importante:** El servicio ML puede no estar disponible. La app web debe funcionar sin √©l.

```python
try:
    prediction = await ml_client.predict_flare(...)
except HTTPException:
    # ML API no disponible
    prediction = None
    # Continuar sin predicci√≥n
    logger.warning("ML service unavailable, continuing without prediction")
```

## üß™ Testing

### Test de integraci√≥n

```python
# tests/test_ml_integration.py
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_ml_api_health():
    """Verificar que ML API est√° disponible."""
    async with AsyncClient(base_url="http://localhost:8001") as client:
        response = await client.get("/health")
        assert response.status_code == 200

@pytest.mark.asyncio
async def test_prediction():
    """Test de predicci√≥n."""
    async with AsyncClient(base_url="http://localhost:8001") as client:
        response = await client.post("/predict", json={
            "symptoms": {...},
            "demographics": {...},
            "history": {...}
        })
        assert response.status_code == 200
        data = response.json()
        assert "prediction" in data
        assert data["prediction"]["flare_risk"] in ["low", "medium", "high"]
```

## üìö Recursos Adicionales

- **Documentaci√≥n interactiva:** http://localhost:8001/docs
- **Ejemplos de uso:** `../scripts/test_api.py`
- **Datos de ejemplo:** `../scripts/api_examples.json`

## ‚ùì FAQ

**P: ¬øQu√© pasa si el servicio ML est√° ca√≠do?**
R: La app web debe continuar funcionando. Simplemente no se generan predicciones.

**P: ¬øDebo guardar las predicciones en mi BD?**
R: S√≠, recomendado. As√≠ tienes hist√≥rico y no dependes 100% del servicio ML.

**P: ¬øPuedo llamar al ML API desde el frontend directamente?**
R: No recomendado. Hazlo desde el backend por seguridad y para manejar errores.

**P: ¬øC√≥mo s√© si una predicci√≥n es nueva o del cache?**
R: Guarda el timestamp cuando llamas a la API. Si hay dos llamadas el mismo d√≠a, usa el cache.

## üìû Soporte

Para dudas sobre la integraci√≥n o errores del servicio ML, contactarme directamente.
