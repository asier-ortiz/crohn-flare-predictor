.PHONY: help install sync clean test format lint serve notebook docker-build docker-run
.PHONY: kill-serve run-notebook-02 run-notebook-03 pipeline reports show-latest-report

# Variables
PYTHON := uv run python
PYTEST := uv run pytest
BLACK := uv run black
FLAKE8 := uv run flake8
JUPYTER := uv run jupyter
UVICORN := uv run uvicorn

help: ## Mostrar este mensaje de ayuda
	@echo "Comandos disponibles:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-20s\033[0m %s\n", $$1, $$2}'

install: ## Instalar uv (gestor de paquetes)
	@echo "Instalando uv..."
	@curl -LsSf https://astral.sh/uv/install.sh | sh || pip install uv

sync: ## Sincronizar e instalar todas las dependencias
	@echo "Instalando dependencias con uv..."
	uv sync
	@echo "âœ… Dependencias instaladas correctamente"

clean: ## Limpiar archivos temporales y cache
	@echo "Limpiando archivos temporales..."
	find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	find . -type d -name ".pytest_cache" -exec rm -rf {} + 2>/dev/null || true
	find . -type d -name ".coverage" -exec rm -rf {} + 2>/dev/null || true
	find . -type d -name "htmlcov" -exec rm -rf {} + 2>/dev/null || true
	find . -type f -name "*.pyc" -delete 2>/dev/null || true
	find . -type d -name "*.egg-info" -exec rm -rf {} + 2>/dev/null || true
	@echo "âœ… Limpieza completada"

test: ## Ejecutar tests de integraciÃ³n (requiere servidor activo en :8001)
	@echo "ğŸ§ª Ejecutando tests de integraciÃ³n..."
	@echo "âš ï¸  AsegÃºrate de que el servidor estÃ© corriendo: make serve"
	@echo ""
	$(PYTEST) --no-cov scripts/test_api.py
	@echo ""
	@echo "âœ… Tests de integraciÃ³n completados"

test-unit: ## Ejecutar tests unitarios con cobertura
	@echo "ğŸ§ª Ejecutando tests unitarios con cobertura..."
	$(PYTEST) --cov=api --cov-report=html --cov-report=term
	@echo "ğŸ“Š Reporte de cobertura generado en htmlcov/index.html"

test-integration: test ## Alias para 'test' (tests de integraciÃ³n)

format: ## Formatear cÃ³digo con Black
	@echo "Formateando cÃ³digo..."
	$(BLACK) api/ scripts/
	@echo "âœ… CÃ³digo formateado"

lint: ## Verificar cÃ³digo con flake8
	@echo "Verificando cÃ³digo..."
	$(FLAKE8) api/ scripts/
	@echo "âœ… VerificaciÃ³n completada"

check: format lint ## Ejecutar formato y lint (sin tests)

check-all: format lint test ## Ejecutar formato, lint y tests (requiere servidor)

serve: ## Iniciar servidor API en modo desarrollo
	@echo "ğŸš€ Iniciando servidor API..."
	@echo "DocumentaciÃ³n disponible en:"
	@echo "  - Swagger UI: http://localhost:8001/docs"
	@echo "  - ReDoc: http://localhost:8001/redoc"
	$(UVICORN) api.app:app --reload --host 0.0.0.0 --port 8001

kill-serve: ## Detener servidor API (libera puerto 8001)
	@echo "ğŸ›‘ Deteniendo servidor en puerto 8001..."
	@lsof -ti :8001 | xargs kill -9 2>/dev/null || echo "âœ… Puerto 8001 ya estÃ¡ libre"

serve-prod: ## Iniciar servidor API en modo producciÃ³n
	@echo "ğŸš€ Iniciando servidor API (producciÃ³n)..."
	$(UVICORN) api.app:app --host 0.0.0.0 --port 8001 --workers 4

test-api: ## Probar endpoints de la API (requiere servidor activo)
	@echo "ğŸ§ª Probando API..."
	$(PYTHON) scripts/test_api.py

test-api-curl: ## Probar API con curl (requiere jq instalado)
	@echo "ğŸ§ª Probando API con curl..."
	./scripts/test_api.sh

evaluate: ## Evaluar precisiÃ³n del modelo (requiere servidor activo)
	@echo "ğŸ”¬ Evaluando modelo..."
	$(PYTHON) scripts/evaluate_model.py

notebook: ## Iniciar Jupyter Notebook (con entorno virtual)
	@echo "ğŸ““ Iniciando Jupyter Notebook..."
	@echo "âš™ï¸  Usando entorno virtual de uv"
	uv run jupyter notebook

lab: ## Iniciar Jupyter Lab (con entorno virtual)
	@echo "ğŸ““ Iniciando Jupyter Lab..."
	@echo "âš™ï¸  Usando entorno virtual de uv"
	uv run jupyter lab

train: ## Ejecutar entrenamiento completo (notebooks 02 y 03)
	@echo "ğŸ¤– Ejecutando pipeline de entrenamiento..."
	@echo "ğŸ“Š Paso 1/2: Feature Engineering (notebook 02)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/02_feature_engineering.ipynb --inplace
	@echo "âœ… Features generadas"
	@echo "ğŸ“Š Paso 2/2: Model Training (notebook 03)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/03_model_training.ipynb --inplace
	@echo "âœ… Modelo entrenado y guardado en models/"

run-notebook-02: ## Ejecutar solo notebook 02 (Feature Engineering)
	@echo "ğŸ“Š Ejecutando Feature Engineering..."
	uv run jupyter nbconvert --to notebook --execute notebooks/02_feature_engineering.ipynb --inplace
	@echo "âœ… Dataset procesado guardado en data/processed/"

run-notebook-03: ## Ejecutar solo notebook 03 (Model Training)
	@echo "ğŸ¤– Ejecutando Model Training..."
	uv run jupyter nbconvert --to notebook --execute notebooks/03_model_training.ipynb --inplace
	@echo "âœ… Modelo guardado en models/"

run-notebook-04: ## Ejecutar solo notebook 04 (Cluster-Stratified Training)
	@echo "ğŸ¯ Ejecutando Cluster-Stratified Model Training..."
	uv run jupyter nbconvert --to notebook --execute notebooks/04_cluster_stratified_training.ipynb --inplace
	@echo "âœ… Modelos por cluster guardados en models/"

train-clusters: ## Entrenar modelos estratificados por cluster (notebooks 01, 02, 03, 04) [LEGACY]
	@echo "ğŸ”¬ Ejecutando pipeline completo con cluster stratification..."
	@echo "ğŸ“Š Paso 1/3: Exploratory Analysis + Clustering (notebook 01)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/01_exploratory_analysis.ipynb --inplace
	@echo "âœ… Clusters generados"
	@echo "ğŸ“Š Paso 2/3: Feature Engineering (notebook 02)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/02_feature_engineering.ipynb --inplace
	@echo "âœ… Features generadas"
	@echo "ğŸ“Š Paso 3/3: Cluster-Stratified Training (notebook 04)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/04_cluster_stratified_training.ipynb --inplace
	@echo "âœ… Modelos por cluster entrenados"
	@echo ""
	@echo "ğŸ“‹ Modelos generados:"
	@echo "  - models/rf_severity_classifier_cluster_0.pkl"
	@echo "  - models/rf_severity_classifier_cluster_1.pkl"
	@echo "  - models/rf_severity_classifier_cluster_2.pkl"
	@echo "  - models/cluster_kmeans.pkl (para inferencia)"
	@echo "  - models/cluster_scaler.pkl"
	@echo ""
	@echo "ğŸ’¡ Siguiente paso: make serve (la API usarÃ¡ automÃ¡ticamente los modelos por cluster)"

run-notebook-05: ## Ejecutar solo notebook 05 (Cluster-Stratified Training CU)
	@echo "ğŸ¯ Ejecutando Cluster-Stratified Model Training (CU)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/05_cluster_stratified_training_cu.ipynb --inplace
	@echo "âœ… Modelos CU por cluster guardados en models/cu/"

train-crohn: ## Entrenar pipeline completo para Crohn (notebooks 01, 02, 03, 04)
	@echo "ğŸ”¬ Pipeline completo: CROHN"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "ğŸ“Š Paso 1/4: Exploratory Analysis + Clustering (Crohn)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/01_exploratory_analysis.ipynb --inplace
	@echo "âœ… Clusters Crohn generados"
	@echo "ğŸ“Š Paso 2/4: Feature Engineering (Crohn)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/02_feature_engineering.ipynb --inplace
	@echo "âœ… Features base generadas"
	@echo "ğŸ“Š Paso 3/4: Advanced Feature Engineering (34 features)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/03_advanced_feature_engineering.ipynb --inplace
	@echo "âœ… Features derivadas generadas (21 nuevas)"
	@echo "ğŸ“Š Paso 4/4: Cluster-Stratified Training (Crohn)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/04_cluster_stratified_training.ipynb --inplace
	@echo "âœ… Modelos Crohn entrenados (34 features)"
	@echo ""
	@echo "ğŸ“‹ Modelos generados en models/crohn/:"
	@echo "  - rf_severity_classifier_cluster_0.pkl"
	@echo "  - rf_severity_classifier_cluster_1.pkl"
	@echo "  - rf_severity_classifier_cluster_2.pkl"
	@echo "  - rf_severity_classifier_global.pkl"
	@echo "  - cluster_kmeans.pkl"
	@echo "  - cluster_scaler.pkl"

train-cu: ## Entrenar pipeline completo para CU (notebooks 01, 02, 03, 05)
	@echo "ğŸ”¬ Pipeline completo: COLITIS ULCEROSA"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "ğŸ“Š Paso 1/4: Exploratory Analysis + Clustering (CU)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/01_exploratory_analysis.ipynb --inplace
	@echo "âœ… Clusters CU generados"
	@echo "ğŸ“Š Paso 2/4: Feature Engineering (CU)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/02_feature_engineering.ipynb --inplace
	@echo "âœ… Features base generadas"
	@echo "ğŸ“Š Paso 3/4: Advanced Feature Engineering (34 features)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/03_advanced_feature_engineering.ipynb --inplace
	@echo "âœ… Features derivadas generadas (21 nuevas)"
	@echo "ğŸ“Š Paso 4/4: Cluster-Stratified Training (CU)..."
	uv run jupyter nbconvert --to notebook --execute notebooks/05_cluster_stratified_training_cu.ipynb --inplace
	@echo "âœ… Modelos CU entrenados (34 features)"
	@echo ""
	@echo "ğŸ“‹ Modelos generados en models/cu/:"
	@echo "  - rf_severity_classifier_cluster_0.pkl"
	@echo "  - rf_severity_classifier_cluster_1.pkl"
	@echo "  - rf_severity_classifier_cluster_2.pkl"
	@echo "  - rf_severity_classifier_global.pkl"
	@echo "  - cluster_kmeans.pkl"
	@echo "  - cluster_scaler.pkl"

train-all: ## Entrenar TODOS los modelos (Crohn + CU)
	@echo "ğŸš€ PIPELINE COMPLETO: CROHN + CU"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "ğŸ“Š Fase 1/2: Entrenando modelos CROHN..."
	@echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
	@$(MAKE) train-crohn
	@echo ""
	@echo "ğŸ“Š Fase 2/2: Entrenando modelos CU..."
	@echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
	@$(MAKE) train-cu
	@echo ""
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "âœ… TODOS LOS MODELOS ENTRENADOS"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "ğŸ“‹ Modelos disponibles:"
	@echo "  ğŸ”¹ CROHN:  models/crohn/rf_severity_classifier_cluster_*.pkl"
	@echo "  ğŸ”¹ CU:     models/cu/rf_severity_classifier_cluster_*.pkl"
	@echo ""
	@echo "ğŸ’¡ Siguiente paso:"
	@echo "   make serve  (La API cargarÃ¡ automÃ¡ticamente ambos tipos)"

setup-data: ## Crear estructura de directorios para datos
	@echo "ğŸ“ Creando estructura de directorios..."
	mkdir -p data/raw data/processed models/crohn models/cu reports/evaluations docs/figures
	@echo "âœ… Directorios creados:"
	@echo "   - data/raw data/processed"
	@echo "   - models/crohn models/cu"
	@echo "   - reports/evaluations"
	@echo "   - docs/figures"
	@echo "âš ï¸  Recuerda descargar el dataset de Kaggle en data/raw/"

dev: sync setup-data ## Setup completo para desarrollo
	@echo "âœ… Entorno de desarrollo listo"
	@echo "Ejecuta 'make notebook' para empezar a analizar datos"
	@echo "Ejecuta 'make serve' para levantar la API"

pipeline: train ## Pipeline completo: entrenar modelo
	@echo ""
	@echo "âœ… Pipeline de entrenamiento completado"
	@echo ""
	@echo "ğŸ“‹ PrÃ³ximos pasos:"
	@echo "  1. make serve          - Iniciar servidor API"
	@echo "  2. make evaluate       - Evaluar modelo (en otra terminal)"
	@echo "  3. make info           - Ver estado del proyecto"

# Docker commands (si decides usar Docker mÃ¡s adelante)
docker-build: ## Construir imagen Docker
	docker build -t crohn-flare-predictor:latest .

docker-run: ## Ejecutar contenedor Docker
	docker run -p 8000:8000 crohn-flare-predictor:latest

# Comandos de utilidad
add: ## Agregar nueva dependencia (uso: make add PKG=nombre-paquete)
	uv add $(PKG)

remove: ## Remover dependencia (uso: make remove PKG=nombre-paquete)
	uv remove $(PKG)

update: ## Actualizar todas las dependencias
	uv sync --upgrade

lock: ## Actualizar lockfile sin instalar
	uv lock

shell: ## Abrir shell en el entorno virtual
	uv run bash

python: ## Abrir Python REPL en el entorno
	$(PYTHON)

info: ## Mostrar informaciÃ³n del proyecto
	@echo "ğŸ“‹ InformaciÃ³n del Proyecto"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "Nombre: crohn-flare-predictor"
	@echo "VersiÃ³n: 1.0.0"
	@echo "Python: $(shell $(PYTHON) --version)"
	@echo ""
	@echo "ğŸ“Š Estado del Modelo:"
	@if [ -f models/rf_severity_classifier.pkl ]; then \
		echo "  âœ… Modelo entrenado: models/rf_severity_classifier.pkl"; \
		ls -lh models/rf_severity_classifier.pkl | awk '{print "     TamaÃ±o: " $$5 " - Modificado: " $$6 " " $$7 " " $$8}'; \
	else \
		echo "  âŒ No hay modelo entrenado (ejecuta: make train)"; \
	fi
	@echo ""
	@echo "ğŸ“‚ Datasets:"
	@if [ -f data/processed/ml_dataset.csv ]; then \
		echo "  âœ… Dataset procesado: data/processed/ml_dataset.csv"; \
		ls -lh data/processed/ml_dataset.csv | awk '{print "     TamaÃ±o: " $$5}'; \
	else \
		echo "  âŒ Dataset no procesado (ejecuta: make run-notebook-02)"; \
	fi
	@echo ""
	@echo "ğŸ“¦ Dependencias principales:"
	@uv pip list | grep -E "(fastapi|scikit-learn|pandas|uvicorn|imbalanced-learn)" || true

reports: ## Ver Ãºltimos reportes de evaluaciÃ³n
	@echo "ğŸ“Š Ãšltimos Reportes de EvaluaciÃ³n"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@if [ -d reports/evaluations ]; then \
		ls -lt reports/evaluations/*.json 2>/dev/null | head -5 | while read -r line; do \
			file=$$(echo $$line | awk '{print $$NF}'); \
			date=$$(echo $$line | awk '{print $$6, $$7, $$8}'); \
			echo "  ğŸ“„ $$(basename $$file) - $$date"; \
		done || echo "  âš ï¸  No hay reportes aÃºn (ejecuta: make evaluate)"; \
	else \
		echo "  âš ï¸  Directorio reports/evaluations no existe"; \
	fi
	@echo ""
	@echo "ğŸ’¡ Para ver un reporte: cat reports/evaluations/evaluation_YYYYMMDD_HHMMSS.json | jq"

show-latest-report: ## Mostrar Ãºltimo reporte de evaluaciÃ³n
	@echo "ğŸ“Š Ãšltimo Reporte de EvaluaciÃ³n"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@latest=$$(ls -t reports/evaluations/*.json 2>/dev/null | head -1); \
	if [ -n "$$latest" ]; then \
		echo "ğŸ“„ Archivo: $$(basename $$latest)"; \
		echo ""; \
		cat "$$latest" | $(PYTHON) -m json.tool; \
	else \
		echo "âš ï¸  No hay reportes disponibles"; \
		echo "ğŸ’¡ Ejecuta: make evaluate (requiere servidor activo)"; \
	fi
